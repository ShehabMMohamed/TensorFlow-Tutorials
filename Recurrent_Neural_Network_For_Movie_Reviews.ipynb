{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Network For Movie Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0JuAmA9WylF0yYJwJDLzW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShehabMMohamed/TensorFlow-Tutorials/blob/main/Recurrent_Neural_Network_For_Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vbHOHuKmdnZ"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1pf7Is3PTuW9iu-VUIs-FhlFBXrsmydhf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTlr1KWLu1m6"
      },
      "source": [
        "**Recurrent Neural Networks**\n",
        "\n",
        "Type of Neural Networks that are good at modelling sequence data. \n",
        "\n",
        "RNNs read the data sequentially in steps before making a prediction or classification. Think of it the same way you say out loud the alphabets, it is easier to remember based on sequence, but if you start saying the alphatets from the letter 'I' it becomes slightly challenging.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Y3U7KJYm5BYONCoEpzJAayTJIYi_YIdn)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Basic Idea Behind an RNN.\n",
        "rnn = RNN()\n",
        "ff = FeedForwardNN()\n",
        "hidden_state = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "for word in input:\n",
        "  output, hidden_state = rnn(word, hidden_state)\n",
        "\n",
        "prediction = ff(output)\n",
        "```\n",
        "\n",
        "\n",
        "**Variations of Recurrent Neural Networks which are capable of learning long term dependencies using mechanism called gates**\n",
        "- Long Short Term Memory (LSTM)\n",
        "- Gated Recurrent Unit (GRU)\n",
        "\n",
        "Gates are a way to optionally let information through. \n",
        "\n",
        "Highly recommended resource for understanding about LSTMs.\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ELnQdguU3F"
      },
      "source": [
        "### Different Sequence Problems used in Recurrent Neural Networks\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1lDjgihc9KOxpTJ2xUFRYlxL3q8IRXuRc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTDWiA5goiE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34edf084-6502-425d-be66-5c152fbbf18c"
      },
      "source": [
        "%tensorflow_version 2.x \n",
        "from keras.datasets import imdb             # one of many available datasets to try out. https://www.tensorflow.org/datasets/catalog/imdb_reviews\n",
        "from keras.preprocessing import sequence    # We need to perform sequence padding to the text to guarantee same input length\n",
        "import tensorflow as tf                     # Deep Learning Library\n",
        "import numpy as np                          # For Fast Matrix Multiplication\n",
        "\n",
        "VOCAB_SIZE = 88584    # Number of Unique Words\n",
        "MAXLEN = 250        # Maximum length of a sentence\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG5uinPuo50f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b51a80-fbcc-4c61-cb59-4f7b41df4eca"
      },
      "source": [
        "print(train_data.shape)\n",
        "type(train_data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EJiWaIBq9Rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c457de47-7ee8-4be1-fb38-c85a6446e96d"
      },
      "source": [
        "# examples of train_data and train_label\n",
        "def show_train_examples(n = 5, flag = True):\n",
        "  for i in range(n):\n",
        "    id = i\n",
        "    print(\"Shape of Input Data [{0}]: {1}\".format(id, len(train_data[id])))\n",
        "    if flag:\n",
        "      print(\"Train: \", train_data[id])\n",
        "      print(\"Label: \", train_labels[id])\n",
        "\n",
        "# Notice the different lengths of the inputs.\n",
        "# The Movie reviews are not of equal length, we need to pad them to a fixed-length vector.\n",
        "show_train_examples(2, True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Input Data [0]: 218\n",
            "Train:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "Label:  1\n",
            "Shape of Input Data [1]: 189\n",
            "Train:  [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
            "Label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS_L5exHqsog"
      },
      "source": [
        "Padding the text to MAXLEN=250"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Wg06kTo72T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a90072-e4b2-45c4-f24c-660e182b5960"
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN) \n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)\n",
        "\n",
        "# Now all reviews are padded to 250 length.\n",
        "show_train_examples(10, False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Input Data [0]: 250\n",
            "Shape of Input Data [1]: 250\n",
            "Shape of Input Data [2]: 250\n",
            "Shape of Input Data [3]: 250\n",
            "Shape of Input Data [4]: 250\n",
            "Shape of Input Data [5]: 250\n",
            "Shape of Input Data [6]: 250\n",
            "Shape of Input Data [7]: 250\n",
            "Shape of Input Data [8]: 250\n",
            "Shape of Input Data [9]: 250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADgpPhSydFIL"
      },
      "source": [
        "**What is Word Embedding?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-IObrcVdIYo"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1y9YsZh5bnxDUH31uy9Y-Hg5Pqt9WnF0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGfpi3_MgdjF"
      },
      "source": [
        "If we have the embedding size of 512, then we have a weighted matrix of (VOCAB_SIZE, EMBEDDING_SIZE) where each row is a word with a vector of 512 numerical values. \n",
        "\n",
        "After training this embedding layer, we can use it as a lookup table! Input a word in the embedding layer and it will output for us its embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbUWfrs6tvVn"
      },
      "source": [
        "Architecture of our Recurrent Neural Network\n",
        "\n",
        "- Input Layer: Embedd the words with vectors of dimensionality 32.\n",
        "- LSTM Layer: Input the embeddings to an LSTM layer with 32 neurons.\n",
        "- Output Layer: output with a single neuron with output value from 0.0 to 1.0 (Sigmoid function)\n",
        "\n",
        "Remember\n",
        "Embedding layer is of shape (Number of Words x Embedding Dimensions)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IdtTfA6qLhL"
      },
      "source": [
        "model = tf.keras.Sequential([ \n",
        "                             tf.keras.layers.Embedding(VOCAB_SIZE, 32),     # Input Layer (1)\n",
        "                             tf.keras.layers.LSTM(32),                      # Hidden Layer\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid') # Output Layer\n",
        "])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8-GjmvJrYS7"
      },
      "source": [
        "View the architecture of our model using summary() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fJ4oG6lq2KK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70867890-bb79-4978-d69f-efd41e15b662"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de_hKWGvupVy"
      },
      "source": [
        "Hyperparameter Tuning\n",
        "\n",
        "\n",
        "What if you train with a different optimizer like Adam?\n",
        "What if you train with more epochs? Is there a stopping condition during training? Observing the validation loss?\n",
        "What if you change the validation split to 15%?\n",
        "Change the RNN architecture?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evA8NHvVq5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c4dd1b-e834-49b1-99d6-f98ccd43501e"
      },
      "source": [
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=3, validation_split=0.2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.4189 - acc: 0.8115 - val_loss: 0.3218 - val_acc: 0.8728\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.2383 - acc: 0.9073 - val_loss: 0.3659 - val_acc: 0.8678\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.1849 - acc: 0.9323 - val_loss: 0.2775 - val_acc: 0.8980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsCvII24vNV7"
      },
      "source": [
        "Test the Accuracy with the test_data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjhAAO89rkxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8b0335-f506-47a0-bfc6-5649dbae0d2a"
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 4s 6ms/step - loss: 0.3213 - acc: 0.8780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G44BAC5grhoZ"
      },
      "source": [
        "Encoding Text function takes an input text and transforms it to an array of numerical values.\n",
        "\n",
        "Decoding Text function takes the array of numerical values and transforms it to an input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Fpzoc-tBiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b609f8e1-48d1-4aa9-d61c-0da05681ac14"
      },
      "source": [
        "# We need this dictionary to encode the text.\n",
        "# This dictionary has the mapping from a word to an integer.\n",
        "word_index = imdb.get_word_index() \n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  # for word in tokens:\n",
        "  #   if word in word_index:\n",
        "  #     tokens.append(word_index[word])\n",
        "  #   else:\n",
        "  #     tokens.append(0)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]   # List Comprehension\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(\"Input Text: \", text)\n",
        "print(\"Encoded Text: \", encoded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Text:  that movie was just amazing, so amazing\n",
            "Encoded Text:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uthsaCqGtjKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6f8a11-779f-405b-a661-01aaec316d67"
      },
      "source": [
        "# We need this dictionary to decode the integers.\n",
        "# This dictionary has the mapping from an integer to a corresponding word.\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}    # Dictionary Comprehension\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text = \"\"\n",
        "  for num in integers:\n",
        "    if num != PAD:\n",
        "      text += reverse_word_index[num] + \" \"\n",
        "  return text[:-1]\n",
        "  \n",
        "print(\"Encoded Text: \" , text)\n",
        "print(\"Decoded Text: \", decode_integers(encoded))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded Text:  that movie was just amazing, so amazing\n",
            "Decoded Text:  that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1DajdBrspCl"
      },
      "source": [
        "After training our network, we are going to test the model with some input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ3Kf_lsuIAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23d9420-1f94-4826-ae1b-1ba1c64b3108"
      },
      "source": [
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1, MAXLEN))    # Batch of Inputs[[review]]\n",
        "  pred[0] = encoded_text          # pred[0] = [review]\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"That movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.77847785]\n",
            "[0.34788582]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYyiocHRvZpm",
        "outputId": "3ac26501-417d-4209-ba79-4837fda80ced"
      },
      "source": [
        "def predict_and_assign_sentiment(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1, MAXLEN))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  review = \"Positive\" if result > 0.5 else \"Negative\"\n",
        "  print(\"Input Text: \", text)\n",
        "  print(\"It is a {0} review!\".format(review))\n",
        "\n",
        "predict_and_assign_sentiment(\"I am not sure about this movie, it was weird for me. I did not like it.\")\n",
        "predict_and_assign_sentiment(\"I will definitely want to see it again!\")\n",
        "predict_and_assign_sentiment(\"This movie was good, lol joking!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Text:  I am not sure about this movie, it was weird for me. I did not like it.\n",
            "It is a Negative review!\n",
            "Input Text:  I will definitely want to see it again!\n",
            "It is a Positive review!\n",
            "Input Text:  This movie was good, lol joking!\n",
            "It is a Negative review!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}